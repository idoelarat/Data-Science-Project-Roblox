{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a209f134",
   "metadata": {},
   "source": [
    "# Games Research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4fe31d",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Imports](#Imports)\n",
    "3. [Data Acquisition](#Data_aqu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b7c15",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"Introduction\"></a>1. Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcb3e5",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"Imports\"></a>2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a1fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (4.9.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: outcome in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: pydot in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from pydot) (3.0.9)\n",
      "Requirement already satisfied: pydotplus in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from pydotplus) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag as HtmlTag\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from random import randint\n",
    "import time\n",
    "from time import sleep\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from typing import Callable, List\n",
    "from numbers import Number\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import Image, display  \n",
    "import pydotplus \n",
    "from scipy import misc\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631c85e",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"Data_aqu\"></a>3. Data Acquisition\n",
    "\n",
    "In order to create a predication model we first needed to gather relevant data.\n",
    "\n",
    "Considering our options of data acquisition sources, we decided to look for the biggest video game digital distribution service and storefront, and scrape data which we thought will be helpful and save it as a dataframe.\n",
    "\n",
    "## The Platform that we choose to get the data:  [Steam](https://store.steampowered.com \"Steam\")\n",
    "\n",
    "We have decided on Steam due to the scale of the community and amount of features provided with each game.\n",
    "We started looking for a way to scrape data from Steam and we immediately faced the issue of having to scroll down to load more games, for that issue we used selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cf110",
   "metadata": {},
   "source": [
    "## Getting Games Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae44be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links scraped: 50\n"
     ]
    }
   ],
   "source": [
    "# Set up the web driver\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://store.steampowered.com/search?category1=998&supportedlang=english&ndl=1\")\n",
    "\n",
    "# Scroll down to load more search results we choose to scroll down 900 times to get near to 50k links\n",
    "for i in range(0):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Extract game links from search results\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "game_tags = soup.find_all(\"a\", {\"class\": \"search_result_row\"})\n",
    "game_links = [tag.get(\"href\") for tag in game_tags]\n",
    "\n",
    "#save game links to df\n",
    "df = pd.DataFrame({\"link_to_game_page\":game_links})\n",
    "\n",
    "# Quit the web driver\n",
    "driver.quit()\n",
    "\n",
    "#print number of links\n",
    "print(f\"Total links scraped: {len(game_links)}\")\n",
    "df.to_csv('game_links.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0b01e",
   "metadata": {},
   "source": [
    "## Read Game links and scrape data from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ebe7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           game_name    game_price  release_date  \\\n",
      "0   Counter-Strike: Global Offensive  Free to Play  21 Aug, 2012   \n",
      "1                      Apex Legends™  Free to Play   4 Nov, 2020   \n",
      "2              Red Dead Redemption 2        230.00   5 Dec, 2019   \n",
      "3                          Destiny 2  Free To Play   1 Oct, 2019   \n",
      "4  Call of Duty®: Modern Warfare® II        395.00  27 Oct, 2022   \n",
      "\n",
      "         publisher              developer all_review_score all_review_count  \\\n",
      "0            Valve                  Valve    Very Positive          7155319   \n",
      "1  Electronic Arts  Respawn Entertainment    Very Positive           638200   \n",
      "2   Rockstar Games         Rockstar Games    Very Positive           361295   \n",
      "3          Bungie                  Bungie    Very Positive           538286   \n",
      "4       Activision          Infinity Ward            Mixed           205084   \n",
      "\n",
      "    genre                                           features  \n",
      "0  Action  [Steam Achievements, Full controller support, ...  \n",
      "1  Action  [Online PvP, Online Co-op, Steam Achievements,...  \n",
      "2  Action  [Single-player, Online PvP, Online Co-op, Stea...  \n",
      "3  Action  [Single-player, Online PvP, Online Co-op, Stea...  \n",
      "4  Action  [Single-player, Online PvP, Online Co-op, Cros...  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file containing the URLs\n",
    "url_df = pd.read_csv('game_links.csv')\n",
    "\n",
    "# Create an empty list to store the scraped data\n",
    "game_data_list = []\n",
    "\n",
    "# Initialize an empty dataframe to store the scraped data\n",
    "game_data_df = pd.DataFrame(columns=['game_name', 'game_price', 'release_date', 'publisher', 'developer', 'all_review_score', 'all_review_count', 'genre', 'features'])\n",
    "\n",
    "# Loop through each URL in the CSV file\n",
    "for index, row in df.iterrows():\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(row['link_to_game_page'])\n",
    "\n",
    "    # Parse the HTML of the response with Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Check if the age verification prompt is present on the page\n",
    "    age_gate = soup.find('div', {'class': 'agegate_text_container'})\n",
    "\n",
    "    # If age verification is required, set the year to 1995 and submit the form\n",
    "    if age_gate:\n",
    "        print(f\"Age verification required for {url}\")\n",
    "        # Extract the form data from the age verification prompt\n",
    "        action_url = age_gate.find('form')['action']\n",
    "        session_id = age_gate.find('input', {'name': 'sessionid'})['value']\n",
    "        age_check = age_gate.find('input', {'name': 'ageDay'})['value']\n",
    "        # Set the year to 1995\n",
    "        age_year = '1995'\n",
    "        # Send a POST request to the age verification form URL with the form data\n",
    "        age_verification_data = {'snr': '1_agecheck_agecheck__age-gate', 'ageDay': age_check, 'ageMonth': '1', 'ageYear': age_year, 'sessionid': session_id}\n",
    "        verification_response = requests.post(action_url, data=age_verification_data)\n",
    "        # Parse the HTML of the age verification response with Beautiful Soup\n",
    "        soup = BeautifulSoup(verification_response.content, \"html.parser\")\n",
    "\n",
    "    # Extract the game name from the page\n",
    "    game_name = soup.select_one('.apphub_AppName').text.strip()\n",
    "\n",
    "    # Extract the game price from the page\n",
    "    discount_element = soup.find(\"div\", {\"class\": \"discount_original_price\"})\n",
    "    if discount_element:\n",
    "        # If on sale, extract the before discounted price\n",
    "        price_element = discount_element\n",
    "    else:\n",
    "        # Otherwise, extract the regular price\n",
    "        price_element = soup.find(\"div\", {\"class\": \"game_purchase_price\"})\n",
    "\n",
    "    # Extract the price text and remove the currency symbol\n",
    "    if price_element:\n",
    "        game_price = price_element.text.strip().replace(\"₪\", \"\")\n",
    "    else:\n",
    "        game_price = \"f2p\"\n",
    "\n",
    "    # Extract the release date from the page\n",
    "    release_date = soup.find('div', {'class': 'date'}).text.strip()\n",
    "\n",
    "    # Extract the publisher and developer from the page\n",
    "    publisher = soup.find('div', {'class': 'dev_row'}).find_next_sibling('div').find('a').text\n",
    "    developer = soup.find('div', {'class': 'dev_row'}).find('a').text\n",
    "\n",
    "    # Extract the all review score and number of all reviews from the page\n",
    "    all_review_score = soup.find_all('span', {'class': 'game_review_summary'})[1].text.strip()\n",
    "    all_review_num = soup.find('meta', {'itemprop': 'reviewCount'})['content']\n",
    "\n",
    "    # Extract the game genres from the page\n",
    "    genres = soup.find('span', {'data-panel': '{\"flow-children\":\"row\"}'}).find('a').text.strip()\n",
    "\n",
    "    # find the div containing the game features\n",
    "    features_div = soup.find('div', {'class': 'game_area_features_list_ctn'})\n",
    "\n",
    "    # find all the game features and store them in a list\n",
    "    features = []\n",
    "    for feature in features_div.find_all('div', {'class': 'label'}):\n",
    "        features.append(feature.text.strip())\n",
    "\n",
    "\n",
    "    # Create a new dataframe with the scraped data\n",
    "    new_game_data_df = pd.DataFrame({\n",
    "        'game_name': [game_name],\n",
    "        'game_price': [game_price],\n",
    "        'release_date': [release_date],\n",
    "        'publisher': [publisher],\n",
    "        'developer': [developer],\n",
    "        'all_review_score': [all_review_score],\n",
    "        'all_review_count': [all_review_num],\n",
    "        'genre': [genres],\n",
    "        'features': [features]\n",
    "    })\n",
    "   # Concatenate the new dataframe with the existing dataframe\n",
    "    if game_data_df.empty:\n",
    "        game_data_df = new_game_data_df\n",
    "    else:\n",
    "        game_data_df = pd.concat([game_data_df, new_game_data_df], ignore_index=True)\n",
    "    \n",
    "# Save the new dataframe to a CSV file\n",
    "game_data_df.to_csv('game_data_df.csv', index=False)\n",
    "\n",
    "# Display the resulting dataframe with the scraped data\n",
    "print(game_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d52b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
