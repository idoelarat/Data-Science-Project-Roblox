{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a209f134",
   "metadata": {},
   "source": [
    "# Games Research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4fe31d",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Imports](#Imports)\n",
    "3. [Data Acquisition](#Data_aqu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b7c15",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"Introduction\"></a>1. Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcb3e5",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"Imports\"></a>2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a1fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (4.9.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: pydot in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from pydot) (3.0.9)\n",
      "Requirement already satisfied: pydotplus in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\ido\\programs\\anaconda3\\lib\\site-packages (from pydotplus) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag as HtmlTag\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from random import randint\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from typing import Callable, List\n",
    "from numbers import Number\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import Image, display  \n",
    "import pydotplus \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy import misc\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631c85e",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"Data_aqu\"></a>3. Data Acquisition\n",
    "\n",
    "In order to create a predication model we first needed to gather relevant data.\n",
    "\n",
    "Considering our options of data acquisition sources, we decided to look for the biggest video game digital distribution service and storefront, and scrape data which we thought will be helpful and save it as a dataframe.\n",
    "\n",
    "## The Platform that we choose to get the data:  [Steam](https://store.steampowered.com \"Steam\")\n",
    "####  \n",
    "<div>\n",
    "<img src=https://www.turn-on.de/media/cache/article_images/media/cms/2017/07/steam-logo.jpg?890000 width=\"300\">\n",
    "</div>\n",
    "\n",
    "We have decided on Steam due to the scale of the community and amount of features provided with each game.\n",
    "We started looking for a way to scrape data from Steam and we immediately faced the issue of having to scroll down to load more games, for that issue we used selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cf110",
   "metadata": {},
   "source": [
    "## Getting Games Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b926d",
   "metadata": {},
   "source": [
    "## Read Game links and scrape data from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b526aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the web driver\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://store.steampowered.com/search?category1=998&supportedlang=english&ndl=1\")\n",
    "\n",
    "# Specify the number of links to scrape\n",
    "num_links_to_scrape = 50\n",
    "\n",
    "# Calculate the number of times to scroll down based on the number of links loaded per scroll\n",
    "num_links_per_scroll = 25\n",
    "num_scrolls = int(num_links_to_scrape / num_links_per_scroll)\n",
    "\n",
    "# Scroll down to load more search results\n",
    "for i in range(num_scrolls):\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "# Extract game links and prices from search results\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Extract price\n",
    "prices = []\n",
    "for game in soup.select(\".search_result_row\"):\n",
    "    price = game.select_one(\".search_price\")\n",
    "    if price and price.select_one(\"strike\"):\n",
    "        price = price.select_one(\"strike\").text.strip()\n",
    "    elif price:\n",
    "        price = price.text.strip()\n",
    "    else:\n",
    "        price = \"N/A\"\n",
    "    if price and \"₪\" in price:\n",
    "        price = price.replace(\"₪\", \"\")\n",
    "    prices.append(price)\n",
    "\n",
    "for price in prices:\n",
    "    if not price:\n",
    "        price = \"N/A\"\n",
    "\n",
    "# Extract game links from search results\n",
    "game_tags = soup.find_all(\"a\", {\"class\": \"search_result_row\"})\n",
    "game_links = [tag.get(\"href\") for tag in game_tags][:num_links_to_scrape]\n",
    "\n",
    "# Save game links and prices to dataframe\n",
    "df = pd.DataFrame({\"link_to_game_page\": game_links, \"price\": prices[:num_links_to_scrape]})\n",
    "\n",
    "# Quit the web driver\n",
    "driver.quit()\n",
    "\n",
    "# Print the number of links and the dataframe\n",
    "print(f\"Total links scraped: {len(game_links)}\")\n",
    "print(df)\n",
    "df.to_csv('game_links_and_prices.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddd416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"game_links_and_prices.csv\")\n",
    "print (len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301de402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2726/10000 [32:52<1:35:01,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://store.steampowered.com/app/221260/Little_Inferno/?snr=1_7_7_230_150_55: ('Connection aborted.', ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))\n",
      "An error occurred while requesting https://store.steampowered.com/app/221260/Little_Inferno/?snr=1_7_7_230_150_55: HTTPSConnectionPool(host='store.steampowered.com', port=443): Max retries exceeded with url: /app/221260/Little_Inferno/?snr=1_7_7_230_150_55 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001ED8D4D7C10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "An error occurred while requesting https://store.steampowered.com/app/221260/Little_Inferno/?snr=1_7_7_230_150_55: HTTPSConnectionPool(host='store.steampowered.com', port=443): Max retries exceeded with url: /app/221260/Little_Inferno/?snr=1_7_7_230_150_55 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001ED8D4D7A30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 2727/10000 [33:07<10:14:24,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://store.steampowered.com/app/702670/Donut_County/?snr=1_7_7_230_150_55: HTTPSConnectionPool(host='store.steampowered.com', port=443): Max retries exceeded with url: /app/702670/Donut_County/?snr=1_7_7_230_150_55 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001ED8C3CB940>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [2:11:26<00:00,  1.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9940\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file containing the URLs\n",
    "data = pd.read_csv('game_links_and_prices.csv')\n",
    "\n",
    "# Create an empty list to store the scraped data\n",
    "game_data_list = []\n",
    "\n",
    "# Initialize an empty dataframe to store the scraped data\n",
    "game_data_df = pd.DataFrame(columns=['game_id',\n",
    "                                     'game_name',\n",
    "                                     'game_price',\n",
    "                                     'release_date',\n",
    "                                     'publisher',\n",
    "                                     'developer',\n",
    "                                     'all_review_score',\n",
    "                                     'all_review_count',\n",
    "                                     'genre',\n",
    "                                     'features'])\n",
    "\n",
    "# Create a session object\n",
    "session = requests.Session()\n",
    "\n",
    "# make all game feature list\n",
    "all_games_features = []\n",
    "\n",
    "# Loop through each URL in the CSV file\n",
    "for index, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    \n",
    "    if 'sub' in row['link_to_game_page']:\n",
    "        continue\n",
    "    \n",
    "    # Extract game price\n",
    "    game_price = row['price']\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            response = session.get(row['link_to_game_page'])\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while requesting {row['link_to_game_page']}: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    # If the request failed, skip to the next URL\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to get data from {row['link_to_game_page']}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Parse the HTML of the response with Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Check if the age verification prompt is present on the page\n",
    "    age_gate = soup.find('div', {'class': 'agegate_text_container'})\n",
    "\n",
    "    # If age verification is required\n",
    "    if age_gate is not None:\n",
    "        inputs = soup.find_all('input')\n",
    "        data = {}\n",
    "        for input in inputs:\n",
    "            name = input.get('name')\n",
    "            value = input.get('value')\n",
    "            if name is not None:\n",
    "                data[name] = value\n",
    "\n",
    "        # Set the age verification value to '1' (indicating over 18)\n",
    "        data['ageDay'] = '1'\n",
    "        data['ageMonth'] = '1'\n",
    "        data['ageYear'] = '1980'\n",
    "\n",
    "        # Send a POST request to the age verification form URL with the input field values\n",
    "        response = requests.post(url, data=data)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        \n",
    "    # Extract the game ID from the page\n",
    "    game_id = soup.find('meta', {'property': 'og:url'})['content'].split('/')[4]    \n",
    "    \n",
    "    # Extract the game name from the page\n",
    "    game_name = soup.find('b', text='Title:').next_sibling.strip().replace(\"™\", \"\").replace(\"®\", \"\").replace(\"’\", \"\")\n",
    "    \n",
    "    # Extract the release date from the page\n",
    "    release_date = soup.find('div', {'class': 'date'})\n",
    "    if release_date is not None:\n",
    "        release_date = release_date.text.strip()\n",
    "    \n",
    "    # Find all the div elements with class \"dev_row\"\n",
    "    dev_rows = soup.find_all(\"div\", {\"class\": \"dev_row\"})\n",
    "\n",
    "    # Loop through the dev rows to find the one containing the \"Publisher:\" text\n",
    "    for dev_row in dev_rows:\n",
    "        if \"Publisher:\" in dev_row.text:\n",
    "            # Extract the publisher name from the next sibling div element with class \"summary\"\n",
    "            publisher = dev_row.find_all(\"a\", href=True)[0].text.strip()\n",
    "            break\n",
    "\n",
    "    # Find all the div elements with class \"dev_row\"\n",
    "    dev_rows = soup.find_all(\"div\", {\"class\": \"dev_row\"})\n",
    "\n",
    "    # Loop through the dev rows to find the one containing the \"Developer:\" text\n",
    "    for dev_row in dev_rows:\n",
    "        if \"Developer:\" in dev_row.text:\n",
    "            # Extract the first developer name from the next sibling div element with class \"summary\"\n",
    "            developer = dev_row.find_all(\"a\", href=True)[0].text.strip()\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Extract the all review score and number of all reviews from the page\n",
    "    all_reviews = soup.find_all('span', {'class': 'game_review_summary'})\n",
    "    if all_reviews and len(all_reviews) > 1:\n",
    "        all_review_score = all_reviews[1].text.strip()\n",
    "    else:\n",
    "        all_review_score = '0'\n",
    "    if soup.find('meta', {'itemprop': 'reviewCount'}):\n",
    "        all_review_num = soup.find('meta', {'itemprop': 'reviewCount'})['content']\n",
    "    else:\n",
    "        all_review_num = '0'\n",
    "\n",
    "\n",
    "    # Extract the game genres from the page\n",
    "    genres_tag = soup.find('span', {'data-panel': '{\"flow-children\":\"row\"}'})\n",
    "    if genres_tag is not None:\n",
    "        genres = genres_tag.find('a').text.strip()\n",
    "    else:\n",
    "        genres = ''\n",
    "\n",
    "    # find the div containing the game features\n",
    "    features_div = soup.find('div', {'class': 'game_area_features_list_ctn'})\n",
    "\n",
    "    # find all the game features and store them in a list\n",
    "    features = []\n",
    "    for feature in features_div.find_all('div', {'class': 'label'}):\n",
    "        features.append(feature.text.strip())\n",
    "    \n",
    "    # Make all features list\n",
    "    all_games_features.append(features)\n",
    "\n",
    "    # Create a new dataframe with the scraped data\n",
    "    new_game_data_df = pd.DataFrame({\n",
    "        'game_id':[game_id],\n",
    "        'game_name': [game_name],\n",
    "        'game_price': [game_price],\n",
    "        'release_date': [release_date],\n",
    "        'publisher': [publisher],\n",
    "        'developer': [developer],\n",
    "        'all_review_score': [all_review_score],\n",
    "        'all_review_count': [all_review_num],\n",
    "        'genre': [genres],\n",
    "        'features': [features]\n",
    "    })\n",
    "   # Concatenate the new dataframe with the existing dataframe\n",
    "    if game_data_df.empty:\n",
    "        game_data_df = new_game_data_df\n",
    "    else:\n",
    "        game_data_df = pd.concat([game_data_df, new_game_data_df], ignore_index=True)\n",
    "    \n",
    "# Save the new dataframe to a CSV file\n",
    "game_data_df.to_csv('game_data_df.csv', index=False)\n",
    "\n",
    "# Display the resulting dataframe with the scraped data\n",
    "print(len(game_data_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d258f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
